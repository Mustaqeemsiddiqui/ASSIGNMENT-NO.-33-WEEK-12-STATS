{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2505eb0e-495a-489c-a9b4-41810fb0fc9c",
   "metadata": {},
   "source": [
    "**Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\n",
    "the validity of the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6cf6b9-a3ee-476e-8f70-bf03a5988cf2",
   "metadata": {},
   "source": [
    "**ANSWER**:----\n",
    "\n",
    "ANOVA (Analysis of Variance) is a statistical technique used to compare means of two or more groups to determine whether there are statistically significant differences among them. However, for ANOVA to provide valid results, certain assumptions must be met. These assumptions include:\n",
    "\n",
    "1. **Independence**: Observations within each group are independent of each other. This means that the values of one observation do not affect or influence the values of another observation.\n",
    "\n",
    "2. **Normality**: The dependent variable (the variable being measured) should follow a normal distribution within each group. This assumption is more critical for smaller sample sizes (typically, n < 30 per group). Violations of normality can impact the accuracy of p-values and confidence intervals.\n",
    "\n",
    "3. **Homogeneity of Variance (Homoscedasticity)**: The variance (spread) of the dependent variable is approximately equal across all groups. This means that the groups should have roughly the same amount of variability. Violations of homogeneity of variance can affect the F-statistic and lead to incorrect conclusions about group differences.\n",
    "\n",
    "Examples of violations of these assumptions that could impact the validity of ANOVA results include:\n",
    "\n",
    "- **Non-normality**: If the data are heavily skewed or have outliers that distort the distribution, ANOVA results might not accurately reflect true group differences. In such cases, transformations of the data (e.g., logarithmic or square root transformations) or non-parametric tests (e.g., Kruskal-Wallis test) might be more appropriate.\n",
    "\n",
    "- **Non-independence**: In cases where observations are not independent (e.g., repeated measures designs where the same subjects are measured multiple times), special adjustments or different statistical techniques (like repeated measures ANOVA) are necessary.\n",
    "\n",
    "- **Violation of homogeneity of variance**: If the assumption of equal variances across groups is violated (often detected using Levene's test or Bartlett's test), the F-statistic in ANOVA may become unreliable. In such cases, using Welch's ANOVA or non-parametric alternatives (like the Welch's ANOVA equivalent, or the Kruskal-Wallis test) might be more appropriate.\n",
    "\n",
    "Addressing these assumptions is crucial to ensure that ANOVA results are reliable and meaningful. When assumptions are violated, alternative approaches or transformations should be considered to obtain valid statistical conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653581cf-5669-4dcc-9476-f8e5b9a771a1",
   "metadata": {},
   "source": [
    "**Q2. What are the three types of ANOVA, and in what situations would each be used?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb006b-1b6f-4c7f-8c01-c9691176d40a",
   "metadata": {},
   "source": [
    "**ANSWER**:---\n",
    "\n",
    "ANOVA (Analysis of Variance) is a statistical technique used to compare means between two or more groups. There are three main types of ANOVA:\n",
    "\n",
    "1. **One-way ANOVA**:\n",
    "   - **Use**: One-way ANOVA is used when you have one independent variable (factor) with two or more levels (groups). It tests whether there are any statistically significant differences among the means of the groups.\n",
    "   - **Example**: A researcher wants to compare the mean test scores of students across three different teaching methods (Method A, Method B, Method C).\n",
    "\n",
    "2. **Two-way ANOVA**:\n",
    "   - **Use**: Two-way ANOVA is used when you have two independent variables (factors) and you want to know whether there is an interaction between them and/or whether each of the main effects (independent variables) has a significant effect on the dependent variable.\n",
    "   - **Example**: A researcher wants to study the effects of both gender (Male vs. Female) and treatment type (Drug A vs. Drug B) on blood pressure readings.\n",
    "\n",
    "3. **Repeated Measures ANOVA**:\n",
    "   - **Use**: Repeated Measures ANOVA is used when measurements are taken on the same subjects at multiple time points or under different conditions. It tests whether there are any statistically significant differences between the means of repeated measurements on the same subjects under different conditions.\n",
    "   - **Example**: A psychologist measures anxiety levels in the same group of participants before and after exposure to a stressor, and then again after a relaxation intervention.\n",
    "\n",
    "**Situational considerations**:\n",
    "\n",
    "- **One-way ANOVA**: This is typically used when you have one categorical independent variable and you want to compare its effect on a continuous dependent variable across multiple groups. It is suitable for designs where you are interested in comparing means across different categories or levels of a single factor.\n",
    "\n",
    "- **Two-way ANOVA**: This is used when you have two categorical independent variables and you want to understand how each independent variable affects the dependent variable, as well as whether there is an interaction effect between the two variables. It is useful for exploring complex relationships between two factors.\n",
    "\n",
    "- **Repeated Measures ANOVA**: This is used when you have a within-subjects design, where each participant is measured multiple times under different conditions or at different time points. It is appropriate when you want to examine changes over time or across conditions within the same group of subjects, controlling for individual differences.\n",
    "\n",
    "Choosing the correct type of ANOVA depends on the specific research question, the design of the study (such as the number of independent variables and their levels), and the nature of the data (whether measurements are independent or within-subjects). Each type of ANOVA provides different insights into the relationships between variables and helps determine whether observed differences are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b1f92-09c6-407b-8dcb-67111844f3e8",
   "metadata": {},
   "source": [
    "**Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701a438-2edb-4a89-916d-0f1fa95c12e9",
   "metadata": {},
   "source": [
    "**ANSWER**:---\n",
    "\n",
    "In ANOVA (Analysis of Variance), the partitioning of variance refers to the division of the total variance observed in the data into different components that are attributable to different sources or factors. This concept is crucial because it helps us understand the relative contributions of these factors to the overall variability in the dependent variable (the variable being measured).\n",
    "\n",
    "Here's how the variance is partitioned in ANOVA:\n",
    "\n",
    "1. **Total Variance**: This is the total variability observed in the dependent variable across all observations or groups. It is typically denoted as \\( SS_{Total} \\) (Sum of Squares Total).\n",
    "\n",
    "2. **Between-Group Variance (or Treatment Variance)**: This component of variance represents the variability between the group means. It measures how much the means of different groups differ from each other. It is denoted as \\( SS_{Between} \\) or \\( SS_{Treatment} \\) (Sum of Squares Between or Sum of Squares Treatment).\n",
    "\n",
    "3. **Within-Group Variance (or Error Variance)**: This component of variance represents the variability within each group. It measures the differences between individual observations and their group mean. It is denoted as \\( SS_{Within} \\), \\( SS_{Error} \\), or \\( SS_{Residual} \\) (Sum of Squares Within, Sum of Squares Error, or Sum of Squares Residual).\n",
    "\n",
    "The partitioning of variance is important for several reasons:\n",
    "\n",
    "- **Identifying Significant Effects**: By partitioning the variance into between-group and within-group components, ANOVA assesses whether the differences observed between group means are larger than would be expected by chance. Significant between-group variance suggests that the independent variable (or variables) have a significant effect on the dependent variable.\n",
    "\n",
    "- **Interpreting F-statistic**: The F-statistic in ANOVA is calculated as the ratio of the between-group variance to the within-group variance. It quantifies the extent to which the group means differ relative to the variability within each group. Understanding how variance is partitioned helps in interpreting the significance of this ratio.\n",
    "\n",
    "- **Assessing Model Fit**: Partitioning variance helps in assessing how well the model (ANOVA model) fits the data. Large between-group variance relative to within-group variance indicates a good fit of the model to explain the differences in group means.\n",
    "\n",
    "- **Understanding Contributions of Factors**: For designs involving multiple factors (such as in factorial ANOVA or repeated measures ANOVA), partitioning variance helps in understanding the unique contributions of each factor and their interactions to the variability in the dependent variable.\n",
    "\n",
    "Overall, understanding the partitioning of variance in ANOVA provides insights into the sources of variability in the data, helps in testing hypotheses about group differences, and guides the interpretation of ANOVA results in terms of the effects of the independent variables on the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271320b3-5469-426e-83ab-822977f4bebf",
   "metadata": {},
   "source": [
    "**Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual\n",
    "sum of squares (SSR) in a one-way ANOVA using Python?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ddafca-9bbd-453a-a9bb-984880d5b51d",
   "metadata": {},
   "source": [
    "**ANSWER**:----\n",
    "\n",
    "In a one-way ANOVA, you can calculate the Total Sum of Squares (SST), Explained Sum of Squares (SSE), and Residual Sum of Squares (SSR) using Python with the help of libraries like NumPy and SciPy. Here's how you can calculate each of these sums of squares:\n",
    "\n",
    "### Step-by-Step Calculation\n",
    "\n",
    "1. **Total Sum of Squares (SST)**:\n",
    "   - SST measures the total variance in the dependent variable (DV).\n",
    "   - Formula: \\( SST = \\sum (Y_i - \\bar{Y})^2 \\)\n",
    "     where \\( Y_i \\) are the individual observations, and \\( \\bar{Y} \\) is the overall mean of all observations.\n",
    "\n",
    "2. **Explained Sum of Squares (SSE)**:\n",
    "   - SSE measures the variance explained by the group means.\n",
    "   - Formula: \\( SSE = \\sum n_j (\\bar{Y}_j - \\bar{Y})^2 \\)\n",
    "     where \\( n_j \\) is the number of observations in the j-th group, \\( \\bar{Y}_j \\) is the mean of the j-th group, and \\( \\bar{Y} \\) is the overall mean.\n",
    "\n",
    "3. **Residual Sum of Squares (SSR)**:\n",
    "   - SSR measures the variance not explained by the group means (i.e., the error variance).\n",
    "   - Formula: \\( SSR = \\sum \\sum (Y_{ij} - \\bar{Y}_j)^2 \\)\n",
    "     where \\( Y_{ij} \\) are the individual observations in the j-th group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d3e92cb-5dc4-469e-b2a6-c57fd9a7840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 487.73333333333335\n",
      "Explained Sum of Squares (SSE): 44.13333333333332\n",
      "Residual Sum of Squares (SSR): 443.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "group1 = np.array([15, 18, 22, 25, 30])\n",
    "group2 = np.array([12, 16, 20, 23, 28])\n",
    "group3 = np.array([10, 14, 18, 21, 26])\n",
    "\n",
    "# Combine all data into one array\n",
    "all_data = np.concatenate([group1, group2, group3])\n",
    "\n",
    "# Compute overall mean\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Compute group means\n",
    "group_means = np.array([np.mean(group1), np.mean(group2), np.mean(group3)])\n",
    "\n",
    "# Compute Total Sum of Squares (SST)\n",
    "SST = np.sum((all_data - overall_mean)**2)\n",
    "\n",
    "# Compute Explained Sum of Squares (SSE)\n",
    "SSE = np.sum(len(group1) * (group_means[0] - overall_mean)**2 +\n",
    "             len(group2) * (group_means[1] - overall_mean)**2 +\n",
    "             len(group3) * (group_means[2] - overall_mean)**2)\n",
    "\n",
    "# Compute Residual Sum of Squares (SSR)\n",
    "SSR = np.sum((group1 - group_means[0])**2) + \\\n",
    "      np.sum((group2 - group_means[1])**2) + \\\n",
    "      np.sum((group3 - group_means[2])**2)\n",
    "\n",
    "print(f\"Total Sum of Squares (SST): {SST}\")\n",
    "print(f\"Explained Sum of Squares (SSE): {SSE}\")\n",
    "print(f\"Residual Sum of Squares (SSR): {SSR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9212116a-efa2-46d1-8425-dcbc57835509",
   "metadata": {},
   "source": [
    "**Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837a322-8678-4097-8536-87d0f3da63cb",
   "metadata": {},
   "source": [
    "**ANSWER**:----\n",
    "\n",
    "In a two-way ANOVA, you may want to calculate both the main effects (for each independent variable/factor) and the interaction effect (the combined effect of the two factors). Here’s how you can calculate these effects using Python, leveraging libraries like NumPy and SciPy for statistical calculations.\n",
    "\n",
    "### Example Scenario\n",
    "Let's consider a hypothetical example where we have two factors: Factor A with 3 levels and Factor B with 4 levels. We want to analyze how these factors influence a dependent variable.\n",
    "\n",
    "### Steps to Calculate Effects\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - You need to have data organized into groups according to the levels of both Factor A and Factor B.\n",
    "\n",
    "2. **Compute Means**:\n",
    "   - Calculate means for each combination of Factor A and Factor B levels.\n",
    "   - Calculate overall mean of the dependent variable.\n",
    "\n",
    "3. **Sum of Squares Computations**:\n",
    "   - Compute the Total Sum of Squares (SST).\n",
    "   - Compute the Sum of Squares for Factor A (SSA).\n",
    "   - Compute the Sum of Squares for Factor B (SSB).\n",
    "   - Compute the Sum of Squares for the Interaction (SSAB).\n",
    "   - Compute the Residual Sum of Squares (SSR).\n",
    "\n",
    "4. **Degrees of Freedom**:\n",
    "   - Calculate degrees of freedom for each component (Factor A, Factor B, Interaction, Residual).\n",
    "\n",
    "5. **Mean Squares**:\n",
    "   - Calculate Mean Squares by dividing Sum of Squares by their respective degrees of freedom.\n",
    "\n",
    "6. **F-statistics**:\n",
    "   - Compute F-statistics for Factor A, Factor B, and Interaction using Mean Squares.\n",
    "\n",
    "7. **Effect Sizes** (optional):\n",
    "   - Calculate effect sizes such as Partial Eta-Squared or Eta-Squared to quantify the strength of the effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd0feec-ea3a-45bf-8f58-6adb19f6c27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor A: F = 1.4721141281967306e+16, p-value = 1.1102230246251565e-16\n",
      "Factor B: F = 4982107087778613.0, p-value = 1.1102230246251565e-16\n",
      "Interaction AB: F = 1322932390540083.0, p-value = 1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "# Assume data is organized in a way that groups correspond to Factor A and Factor B levels\n",
    "# Here, we create example data for demonstration purposes\n",
    "factor_a = np.repeat([1, 2, 3], 4)  # Factor A with 3 levels, each repeated 4 times\n",
    "factor_b = np.tile([1, 2, 3, 4], 3)  # Factor B with 4 levels, each repeated 3 times\n",
    "dependent_var = np.array([10, 12, 15, 11, 14, 13, 18, 20, 17, 16, 19, 22])\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = np.mean(dependent_var)\n",
    "\n",
    "# Calculate group means\n",
    "group_means = np.empty((3, 4))\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        group_means[i, j] = np.mean(dependent_var[(factor_a == i + 1) & (factor_b == j + 1)])\n",
    "\n",
    "# Compute Total Sum of Squares (SST)\n",
    "SST = np.sum((dependent_var - overall_mean)**2)\n",
    "\n",
    "# Compute Sum of Squares for Factor A (SSA)\n",
    "SSA = np.sum(4 * (np.mean(group_means, axis=1) - overall_mean)**2)\n",
    "\n",
    "# Compute Sum of Squares for Factor B (SSB)\n",
    "SSB = np.sum(3 * (np.mean(group_means, axis=0) - overall_mean)**2)\n",
    "\n",
    "# Compute Sum of Squares for Interaction (SSAB)\n",
    "SSAB = np.sum((group_means - np.mean(group_means, axis=1, keepdims=True) - \n",
    "               np.mean(group_means, axis=0, keepdims=True) + overall_mean)**2)\n",
    "\n",
    "# Compute Residual Sum of Squares (SSR)\n",
    "SSR = SST - SSA - SSB - SSAB\n",
    "\n",
    "# Degrees of freedom\n",
    "df_a = 2  # Degrees of freedom for Factor A (3 levels - 1)\n",
    "df_b = 3  # Degrees of freedom for Factor B (4 levels - 1)\n",
    "df_ab = 6  # Degrees of freedom for Interaction (df_a * df_b)\n",
    "df_w = 6  # Residual degrees of freedom (total observations - total factors)\n",
    "\n",
    "# Mean Squares\n",
    "MSA = SSA / df_a\n",
    "MSB = SSB / df_b\n",
    "MSAB = SSAB / df_ab\n",
    "MSR = SSR / df_w\n",
    "\n",
    "# F-statistics\n",
    "F_A = MSA / MSR\n",
    "F_B = MSB / MSR\n",
    "F_AB = MSAB / MSR\n",
    "\n",
    "# Print results\n",
    "print(f\"Factor A: F = {F_A}, p-value = {1 - stats.f.cdf(F_A, df_a, df_w)}\")\n",
    "print(f\"Factor B: F = {F_B}, p-value = {1 - stats.f.cdf(F_B, df_b, df_w)}\")\n",
    "print(f\"Interaction AB: F = {F_AB}, p-value = {1 - stats.f.cdf(F_AB, df_ab, df_w)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96f3ef-a1f1-4c58-bc9b-84392dc18a2b",
   "metadata": {},
   "source": [
    "**Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "What can you conclude about the differences between the groups, and how would you interpret these\n",
    "results?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38119ce9-8067-4553-829a-112c6363a3fa",
   "metadata": {},
   "source": [
    "**ANSWER**:---\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "1. **F-statistic (5.23)**:\n",
    "   - The F-statistic is a ratio of the variance between groups to the variance within groups. A higher F-statistic indicates that the differences between group means are larger relative to the variability within each group.\n",
    "\n",
    "2. **P-value (0.02)**:\n",
    "   - The p-value associated with the F-statistic (0.02) is below the conventional significance level of 0.05. This indicates that the observed differences between the group means are statistically significant.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Based on the F-statistic and p-value:\n",
    "\n",
    "- **Statistical Significance**: Since the p-value (0.02) is less than the significance level (typically 0.05), we reject the null hypothesis. The null hypothesis in ANOVA states that there are no significant differences between the means of the groups. Therefore, we conclude that there are statistically significant differences between at least two of the groups.\n",
    "\n",
    "- **Group Differences**: The significant F-statistic suggests that there are differences in the means of the groups being compared. In practical terms, this means that the factor (independent variable) under consideration (e.g., different treatments, conditions, or categories) has a significant effect on the dependent variable.\n",
    "\n",
    "### Practical Interpretation:\n",
    "\n",
    "- **Post-hoc Tests**: After finding a significant result in ANOVA, it is common practice to perform post-hoc tests (e.g., Tukey's HSD, Bonferroni, or Dunnett's test) to determine which specific groups differ from each other. These tests help identify pairwise differences and provide more detailed insights into the nature of the group differences.\n",
    "\n",
    "- **Effect Size**: Additionally, it is useful to calculate effect size measures (e.g., eta-squared or partial eta-squared) to quantify the magnitude of the differences between groups. Effect size measures provide a clearer understanding of the practical significance of the findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af43a9-c8db-4e21-8eff-7713975cd7b1",
   "metadata": {},
   "source": [
    "**Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\n",
    "consequences of using different methods to handle missing data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf9388a-b215-4b85-9434-06264abdf377",
   "metadata": {},
   "source": [
    "**ANSWER**:----\n",
    "\n",
    "Handling missing data in a repeated measures ANOVA is crucial to ensure the validity and reliability of your statistical analysis. Here’s how WE can approach handling missing data and the potential consequences of different methods:\n",
    "\n",
    "### Handling Missing Data:\n",
    "\n",
    "1. **Identify and Understand the Missing Data Pattern**:\n",
    "   - First, identify the pattern of missing data (e.g., completely at random, missing at random, or not at random). This helps in selecting appropriate methods for handling missing data.\n",
    "\n",
    "2. **Listwise Deletion (Complete Case Analysis)**:\n",
    "   - In this method, cases with any missing data across any variable are excluded from the analysis. This is simple but reduces sample size and may introduce bias if missingness is related to the dependent variable or other factors.\n",
    "\n",
    "3. **Pairwise Deletion (Available Case Analysis)**:\n",
    "   - This method includes cases for which data are available on at least one variable. It retains more data than listwise deletion but can lead to biased estimates if data are not missing completely at random.\n",
    "\n",
    "4. **Imputation Methods**:\n",
    "   - **Mean Imputation**: Replace missing values with the mean of the observed values for that variable. This maintains sample size but can distort variance estimates and correlations.\n",
    "   - **Regression Imputation**: Predict missing values based on other variables that are correlated with the missing variable. This method can provide more accurate estimates but assumes the imputation model is correctly specified.\n",
    "   - **Multiple Imputation**: Generate multiple plausible values for each missing data point to account for uncertainty in imputation. This method preserves variability and produces more robust estimates but requires appropriate software and assumptions about the missing data mechanism.\n",
    "\n",
    "5. **Model-Based Methods**:\n",
    "   - Use advanced statistical models (e.g., mixed-effects models) that can handle missing data directly by estimating parameters using all available data, including incomplete cases. These methods provide unbiased estimates under the assumption of missing at random (MAR) and are increasingly preferred when data are missing non-randomly.\n",
    "\n",
    "### Potential Consequences of Different Methods:\n",
    "\n",
    "- **Bias**: Listwise and pairwise deletion can introduce bias if missingness is related to the outcome or other variables in the model.\n",
    "- **Loss of Power**: Deleting cases reduces sample size, which can decrease statistical power to detect true effects.\n",
    "- **Incorrect Estimates**: Mean imputation can distort relationships and variability, leading to incorrect parameter estimates.\n",
    "- **Underestimation of Variability**: Methods that do not properly account for missing data (e.g., deletion or simple imputation) can underestimate the variability in the data, affecting standard errors and hypothesis tests.\n",
    "- **Inflated Type I Error**: Improper handling of missing data can lead to inflated Type I error rates (false positives) if the missing data mechanism is not accounted for correctly.\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- **Understand Missing Data Mechanism**: Determine whether data are missing completely at random, at random, or not at random to inform appropriate handling methods.\n",
    "- **Use Multiple Imputation or Model-Based Methods**: Prefer multiple imputation or model-based approaches when feasible, as they can provide more reliable estimates and preserve statistical power.\n",
    "- **Sensitivity Analysis**: Perform sensitivity analyses to assess the robustness of results to different assumptions about the missing data mechanism and handling methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59002d-aaba-43f6-b52a-02f28deba649",
   "metadata": {},
   "source": [
    "**Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "an example of a situation where a post-hoc test might be necessary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7ac844-9472-4c7e-ab33-0a412efde7e8",
   "metadata": {},
   "source": [
    "**ANSWER**:---\n",
    "\n",
    "After conducting an ANOVA (Analysis of Variance) and finding a significant result, post-hoc tests are often used to further investigate and determine which specific groups differ from each other. Here are some common post-hoc tests used after ANOVA, along with scenarios where each might be appropriate:\n",
    "\n",
    "### Common Post-Hoc Tests:\n",
    "\n",
    "1. **Tukey's Honestly Significant Difference (HSD) Test**:\n",
    "   - **Use**: Tukey's HSD test is widely used when comparing all possible pairs of means from multiple groups. It controls the overall Type I error rate and is appropriate when you have equal sample sizes and equal variances across groups.\n",
    "   - **Example**: After conducting a one-way ANOVA comparing mean exam scores among three different teaching methods (Method A, Method B, Method C), Tukey's HSD test can be used to determine which specific pairs of methods have significantly different mean scores.\n",
    "\n",
    "2. **Bonferroni Correction**:\n",
    "   - **Use**: Bonferroni correction adjusts the significance level for multiple comparisons. It is conservative but effective in controlling the family-wise error rate.\n",
    "   - **Example**: Suppose you conduct multiple pairwise comparisons between different treatment groups after a factorial ANOVA. You can apply Bonferroni correction to maintain an overall significance level of 0.05 across all comparisons.\n",
    "\n",
    "3. **Dunnett's Test**:\n",
    "   - **Use**: Dunnett's test compares each treatment group mean with a control group mean. It is useful when you have a control group and want to test if other groups differ significantly from this control group.\n",
    "   - **Example**: In a clinical trial comparing the effectiveness of three different drugs (Drug A, Drug B, Drug C) against a placebo (control group) for pain relief, Dunnett's test can be used to compare each drug's effectiveness relative to the placebo.\n",
    "\n",
    "4. **Sidak Correction**:\n",
    "   - **Use**: Similar to Bonferroni correction, Sidak correction adjusts for multiple comparisons but tends to be less conservative. It is useful when conducting multiple tests to maintain a family-wise error rate.\n",
    "   - **Example**: When performing multiple comparisons among several groups in a study on the effects of different diets (Diet A, Diet B, Diet C) on weight loss, Sidak correction can be applied to adjust the significance level appropriately.\n",
    "\n",
    "5. **Holm-Bonferroni Method**:\n",
    "   - **Use**: The Holm-Bonferroni method is a step-down procedure that adjusts the p-values sequentially, starting with the most significant comparison. It provides a compromise between the stringency of Bonferroni correction and the less conservative approaches.\n",
    "   - **Example**: After a two-way ANOVA examining the effects of both temperature (low, medium, high) and humidity (low, medium, high) on plant growth, the Holm-Bonferroni method can be used to identify significant interactions between specific temperature and humidity levels.\n",
    "\n",
    "### Example Scenario:\n",
    "\n",
    "Imagine a study where researchers investigate the impact of different study techniques (Technique A, Technique B, Technique C) on exam performance among college students. After conducting a one-way ANOVA, they find a significant difference in mean exam scores across the three techniques (p < 0.05).\n",
    "\n",
    "**Post-hoc Test Application**:\n",
    "- To pinpoint which specific techniques lead to significantly different exam scores, the researchers would conduct Tukey's HSD test. This test would compare the mean exam scores of Technique A vs. Technique B, Technique A vs. Technique C, and Technique B vs. Technique C, providing insights into which pairs of techniques differ significantly.\n",
    "\n",
    "In this scenario, Tukey's HSD test is appropriate because it compares all pairs of means and controls for multiple comparisons, ensuring that the identified differences are statistically significant.\n",
    "\n",
    "Choosing the right post-hoc test depends on the structure of your study, the nature of your comparisons, and whether you have a priori hypotheses about specific group differences. Each test has strengths and limitations, so it's essential to select the most suitable test based on your research design and objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66309c4c-cb93-4847-b4ec-5ece897d00be",
   "metadata": {},
   "source": [
    "**Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "Report the F-statistic and p-value, and interpret the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145961b-3f14-4a10-bcc8-6f677c3aaf10",
   "metadata": {},
   "source": [
    "**ANSWER**:----\n",
    "\n",
    "To conduct a one-way ANOVA in Python to determine if there are significant differences between the mean weight loss of three diets (A, B, and C), you can use the `scipy.stats` module, which provides a convenient function `f_oneway` for ANOVA calculations. Here's how you can perform the analysis step-by-step:\n",
    "\n",
    "### Step-by-Step Python Implementation\n",
    "\n",
    "1. **Import Required Libraries**:\n",
    "   - We'll use `numpy` for numerical operations and `scipy.stats` for the ANOVA test.\n",
    "\n",
    "2. **Define the Data**:\n",
    "   - Assume we have weight loss data for each diet stored in separate arrays (`weight_loss_A`, `weight_loss_B`, `weight_loss_C`).\n",
    "\n",
    "3. **Perform ANOVA**:\n",
    "   - Use `scipy.stats.f_oneway` to compute the F-statistic and p-value.\n",
    "\n",
    "4. **Interpret the Results**:\n",
    "   - Based on the F-statistic and p-value, interpret whether there are significant differences between the mean weight loss of the three diets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a15c9d4-835d-41c5-9bc7-8f6b21f308b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA results:\n",
      "F-statistic: 15.446984491671449\n",
      "P-value: 2.617777666165596e-06\n",
      "The one-way ANOVA result is significant, indicating there are significant differences between the mean weight loss of the three diets.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Example data (replace with actual weight loss data)\n",
    "weight_loss_A = np.array([3.2, 4.5, 2.8, 5.1, 3.9, 4.2, 3.7, 2.5, 4.8, 3.3,\n",
    "                          3.6, 4.1, 2.9, 3.4, 4.7, 3.1, 2.6, 3.8, 4.0, 4.3,\n",
    "                          3.5, 4.4, 2.7, 3.0, 4.6])\n",
    "weight_loss_B = np.array([2.5, 3.8, 2.1, 4.4, 3.2, 3.5, 3.0, 1.8, 4.1, 2.6,\n",
    "                          2.9, 3.4, 2.2, 2.7, 3.6, 2.0, 1.5, 3.1, 3.3, 3.7,\n",
    "                          2.8, 3.9, 2.0, 2.3, 3.6])\n",
    "weight_loss_C = np.array([2.0, 3.3, 1.6, 3.9, 2.7, 3.0, 2.5, 1.3, 3.6, 2.1,\n",
    "                          2.4, 2.9, 1.7, 2.2, 3.5, 1.9, 1.4, 2.8, 3.0, 3.2,\n",
    "                          2.3, 3.4, 1.5, 1.8, 3.1])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(weight_loss_A, weight_loss_B, weight_loss_C)\n",
    "\n",
    "# Print results\n",
    "print(f\"One-way ANOVA results:\")\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"The one-way ANOVA result is significant, indicating there are significant differences between the mean weight loss of the three diets.\")\n",
    "else:\n",
    "    print(\"The one-way ANOVA result is not significant, indicating there are no significant differences between the mean weight loss of the three diets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b824827c-3752-4de8-a878-b0634254923b",
   "metadata": {},
   "source": [
    "**Q10. A company wants to know if there are any significant differences in the average time it takes to\n",
    "complete a task using three different software programs: Program A, Program B, and Program C. They\n",
    "randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
    "complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\n",
    "interaction effects between the software programs and employee experience level (novice vs.\n",
    "experienced). Report the F-statistics and p-values, and interpret the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f8be2-9a6b-42e8-8aaa-fa4d7d57e1d5",
   "metadata": {},
   "source": [
    "**ANSWER**:----\n",
    "\n",
    "To conduct a two-way ANOVA in Python to analyze the effects of software programs (Program A, Program B, Program C) and employee experience level (novice vs. experienced) on the time to complete a task, WE can use the `statsmodels` library, which provides a comprehensive framework for statistical modeling in Python. Here’s how WE can perform the analysis step-by-step:\n",
    "\n",
    "### Step-by-Step Python Implementation\n",
    "\n",
    "1. **Import Required Libraries**:\n",
    "   - `numpy` for numerical operations.\n",
    "   - `pandas` for data manipulation.\n",
    "   - `statsmodels` for conducting the ANOVA.\n",
    "\n",
    "2. **Create Data**:\n",
    "   - Generate or load data where each row represents an employee, and columns represent the software program used (`Program`) and experience level (`Experience`), along with the time taken to complete the task (`Time`).\n",
    "\n",
    "3. **Perform Two-Way ANOVA**:\n",
    "   - Use `ols` from `statsmodels.formula.api` to create a model formula and then fit the model.\n",
    "   - Use `anova_lm` from `statsmodels.stats.anova` to obtain ANOVA table and results.\n",
    "\n",
    "4. **Interpret the Results**:\n",
    "   - Analyze the F-statistics, p-values, and interpret whether there are significant main effects of software programs, employee experience level, and interaction effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b216a071-ac63-4e70-ab48-d46b861280f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-way ANOVA results:\n",
      "                             sum_sq    df         F    PR(>F)\n",
      "C(Program)                 1.035327   2.0  0.136986  0.872659\n",
      "C(Experience)              0.521940   1.0  0.138118  0.713420\n",
      "C(Program):C(Experience)   2.683910   2.0  0.355113  0.704716\n",
      "Residual                  90.694755  24.0       NaN       NaN\n",
      "\n",
      "Interpretation:\n",
      "There is no significant main effect of software programs on the time taken.\n",
      "There is no significant main effect of employee experience level on the time taken.\n",
      "There is no significant interaction effect between software programs and employee experience level.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# Example data (replace with actual data)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate example data\n",
    "n = 30\n",
    "programs = np.random.choice(['A', 'B', 'C'], n)\n",
    "experience = np.random.choice(['novice', 'experienced'], n)\n",
    "times = np.random.normal(loc=10, scale=2, size=n)  # Example times taken (normally distributed)\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({'Program': programs, 'Experience': experience, 'Time': times})\n",
    "\n",
    "# Convert Experience to categorical variable\n",
    "data['Experience'] = pd.Categorical(data['Experience'], categories=['novice', 'experienced'])\n",
    "\n",
    "# Fit the ANOVA model\n",
    "formula = 'Time ~ C(Program) + C(Experience) + C(Program):C(Experience)'\n",
    "model = ols(formula, data).fit()\n",
    "anova_results = anova_lm(model, typ=2)\n",
    "\n",
    "# Print ANOVA table\n",
    "print(\"Two-way ANOVA results:\")\n",
    "print(anova_results)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "print(\"\\nInterpretation:\")\n",
    "if anova_results['PR(>F)']['C(Program)'] < alpha:\n",
    "    print(\"There is a significant main effect of software programs on the time taken.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of software programs on the time taken.\")\n",
    "\n",
    "if anova_results['PR(>F)']['C(Experience)'] < alpha:\n",
    "    print(\"There is a significant main effect of employee experience level on the time taken.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of employee experience level on the time taken.\")\n",
    "\n",
    "if anova_results['PR(>F)']['C(Program):C(Experience)'] < alpha:\n",
    "    print(\"There is a significant interaction effect between software programs and employee experience level.\")\n",
    "else:\n",
    "    print(\"There is no significant interaction effect between software programs and employee experience level.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6ab08-7779-4b2f-8634-4533021a98e2",
   "metadata": {},
   "source": [
    "**Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "group(s) differ significantly from each other.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285071f5-299e-489f-9e05-ec62a98026e0",
   "metadata": {},
   "source": [
    "**ANSWER**:----\n",
    "\n",
    "\n",
    "### Step-by-Step Python Implementation\n",
    "\n",
    "1. **Import Required Libraries**:\n",
    "   - `numpy` for numerical operations.\n",
    "   - `scipy.stats` for statistical tests.\n",
    "\n",
    "2. **Generate or Load Data**:\n",
    "   - Simulate or load the test scores for the control and experimental groups.\n",
    "\n",
    "3. **Perform Two-Sample T-Test**:\n",
    "   - Use `scipy.stats.ttest_ind` to perform the two-sample t-test to compare means of the two groups.\n",
    "\n",
    "4. **Post-hoc Test (if significant)**:\n",
    "   - Depending on the results of the t-test, conduct a post-hoc test (e.g., Tukey's HSD, Bonferroni, etc.) to determine which group(s) differ significantly.\n",
    "\n",
    "\n",
    "- **Interpretation**:\n",
    "  - Compare the computed p-value to a significance level (alpha = 0.05) to determine if there is a significant difference in test scores between the two groups.\n",
    "  - If the p-value is less than alpha, conclude that there is a significant difference in test scores.\n",
    "  - If significant, proceed with a post-hoc test (here demonstrated with Tukey's HSD test) to identify which specific groups (if any) differ significantly.\n",
    "\n",
    "- **Post-hoc Test**: If the t-test indicates significant differences, use appropriate post-hoc tests (like Tukey's HSD, Bonferroni, etc.) to further investigate and compare specific group differences.\n",
    "\n",
    "This approach provides a structured way to assess whether the new teaching method leads to significantly different test scores compared to the traditional method and to identify any specific group differences if they exist. Adjust the data and post-hoc test methods based on your specific study design and hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c0f7d3-381c-4224-8429-b0b03967048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test results:\n",
      "T-statistic: -4.316398519082441\n",
      "P-value: 2.5039591073846333e-05\n",
      "There is a significant difference in test scores between the control and experimental groups.\n",
      "\n",
      "Tukey's HSD test results:\n",
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "========================================================\n",
      " group1    group2    meandiff p-adj lower  upper  reject\n",
      "--------------------------------------------------------\n",
      "Control Experimental   6.3061   0.0 3.4251 9.1872   True\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Example data (replace with actual test scores)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate example data: test scores\n",
    "control_scores = np.random.normal(loc=70, scale=10, size=100)   # Control group (traditional method)\n",
    "experimental_scores = np.random.normal(loc=75, scale=12, size=100)  # Experimental group (new method)\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "# Print t-test results\n",
    "print(f\"Two-sample t-test results:\")\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in test scores between the control and experimental groups.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in test scores between the control and experimental groups.\")\n",
    "\n",
    "# Perform post-hoc test (if significant)\n",
    "if p_value < alpha:\n",
    "    # Example of using Tukey's HSD test as a post-hoc test\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "    \n",
    "    # Combine data for Tukey's HSD test\n",
    "    all_scores = np.concatenate([control_scores, experimental_scores])\n",
    "    groups = ['Control'] * len(control_scores) + ['Experimental'] * len(experimental_scores)\n",
    "    \n",
    "    # Perform Tukey's HSD test\n",
    "    tukey_results = pairwise_tukeyhsd(all_scores, groups, alpha=0.05)\n",
    "    print(\"\\nTukey's HSD test results:\")\n",
    "    print(tukey_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc470c-50e5-4575-bb89-296ddf4384e5",
   "metadata": {},
   "source": [
    "**Q12. A researcher wants to know if there are any significant differences in the average daily sales of three\n",
    "retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store\n",
    "on those days. Conduct a repeated measures ANOVA using Python to determine if there are any**\n",
    "\n",
    "**significant differences in sales between the three stores. If the results are significant, follow up with a post-\n",
    "hoc test to determine which store(s) differ significantly from each other.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def399e-8c9b-411f-bf2f-3e793efdd2a5",
   "metadata": {},
   "source": [
    "**ANSWER**:----\n",
    "\n",
    "To analyze whether there are significant differences in average daily sales between three retail stores (Store A, Store B, and Store C) using repeated measures ANOVA in Python, WE can use the `statsmodels` library, which provides functionality for conducting ANOVA including repeated measures designs. Here’s how WE  can perform these analyses step-by-step:\n",
    "\n",
    "### Step-by-Step Python Implementation\n",
    "\n",
    "1. **Import Required Libraries**:\n",
    "   - `numpy` for numerical operations.\n",
    "   - `pandas` for data manipulation.\n",
    "   - `statsmodels` for statistical modeling.\n",
    "\n",
    "2. **Generate or Load Data**:\n",
    "   - Simulate or load the daily sales data for each store over the 30 selected days.\n",
    "\n",
    "3. **Prepare Data for Repeated Measures ANOVA**:\n",
    "   - Convert the data into a format suitable for repeated measures analysis using `statsmodels`.\n",
    "\n",
    "4. **Perform Repeated Measures ANOVA**:\n",
    "   - Use `statsmodels` to fit the repeated measures ANOVA model.\n",
    "   - Extract relevant statistics including F-values and p-values.\n",
    "\n",
    "5. **Post-hoc Test (if significant)**:\n",
    "   - If the ANOVA indicates significant differences, follow up with a post-hoc test (e.g., Tukey's HSD) to determine which store(s) differ significantly.\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- **Data Generation**: In this example, `sales_A`, `sales_B`, and `sales_C` are generated randomly for demonstration purposes. Replace these arrays with your actual daily sales data for each store.\n",
    "  \n",
    "- **DataFrame Construction**: Construct a DataFrame (`data`) where each row represents a day's sales data for one store (`Store`) over 30 days (`Day`).\n",
    "\n",
    "- **Repeated Measures ANOVA**: Use `AnovaRM` from `statsmodels.stats.anova` to fit the repeated measures ANOVA model. Specify `Sales` as the dependent variable, `Day` as the repeated measures variable, and `Store` as the within-subject factor.\n",
    "\n",
    "- **Interpretation**: \n",
    "  - Extract and interpret the F-values and p-values from the ANOVA results to determine if there are significant differences in daily sales between the stores.\n",
    "  - If significant, proceed with a post-hoc test (here demonstrated with Tukey's HSD test) to identify which specific stores differ significantly.\n",
    "\n",
    "- **Post-hoc Test**: If the repeated measures ANOVA indicates significant differences, use appropriate post-hoc tests (like Tukey's HSD, Bonferroni, etc.) to further investigate and compare specific store differences.\n",
    "\n",
    "This approach provides a structured way to assess whether there are significant differences in average daily sales between the three retail stores using repeated measures ANOVA and to identify any specific store differences if they exist. Adjust the data and post-hoc test methods based on our specific study design and hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14232265-932e-4d90-af44-74c4df8a515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated measures ANOVA results:\n",
      "               Anova\n",
      "===================================\n",
      "      F Value Num DF  Den DF Pr > F\n",
      "-----------------------------------\n",
      "Store  8.1518 2.0000 58.0000 0.0008\n",
      "===================================\n",
      "\n",
      "\n",
      "Interpretation:\n",
      "There is a significant difference in daily sales between at least two of the three stores.\n",
      "\n",
      "Tukey's HSD test results:\n",
      " Multiple Comparison of Means - Tukey HSD, FWER=0.05  \n",
      "======================================================\n",
      "group1 group2 meandiff p-adj   lower    upper   reject\n",
      "------------------------------------------------------\n",
      "     A      B 104.2752 0.0006  40.2031 168.3473   True\n",
      "     A      C   70.232 0.0282   6.1599 134.3041   True\n",
      "     B      C -34.0432 0.4176 -98.1153  30.0289  False\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "\n",
    "# Example data (replace with actual daily sales data)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate example data: daily sales for 30 days\n",
    "days = np.arange(1, 31)\n",
    "sales_A = np.random.normal(loc=1000, scale=100, size=30)  # Store A\n",
    "sales_B = np.random.normal(loc=1100, scale=120, size=30)  # Store B\n",
    "sales_C = np.random.normal(loc=1050, scale=110, size=30)  # Store C\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Day': np.tile(days, 3),\n",
    "    'Store': np.repeat(['A', 'B', 'C'], 30),\n",
    "    'Sales': np.concatenate([sales_A, sales_B, sales_C])\n",
    "})\n",
    "\n",
    "# Convert Store to categorical variable\n",
    "data['Store'] = pd.Categorical(data['Store'], categories=['A', 'B', 'C'])\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "# Using AnovaRM from statsmodels\n",
    "anova_rm = AnovaRM(data, 'Sales', 'Day', within=['Store'])\n",
    "results = anova_rm.fit()\n",
    "\n",
    "# Print ANOVA table\n",
    "print(\"Repeated measures ANOVA results:\")\n",
    "print(results)\n",
    "\n",
    "# Extract F-values and p-values\n",
    "f_statistic = results.anova_table['F Value'][0]\n",
    "p_value = results.anova_table['Pr > F'][0]\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "print(\"\\nInterpretation:\")\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in daily sales between at least two of the three stores.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in daily sales between the three stores.\")\n",
    "\n",
    "# Perform post-hoc test (if significant)\n",
    "if p_value < alpha:\n",
    "    # Example of using Tukey's HSD test as a post-hoc test\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "    \n",
    "    # Perform Tukey's HSD test\n",
    "    tukey_results = pairwise_tukeyhsd(data['Sales'], data['Store'], alpha=0.05)\n",
    "    print(\"\\nTukey's HSD test results:\")\n",
    "    print(tukey_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ff351-6b67-44b3-84a7-27e485324927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
